# LocalLLMChat

LocalLLMChat is a lightweight desktop chat application written in Java that lets you talk to a locally hosted LLM (such as LLaMA) through a simple graphical interface. Itâ€™s designed to be straightforward, fast, and easy to modify for experimentation and personal use.

This project was built as a hands-on learning tool for working with local language models, Java Swing UI, and API communication â€” while keeping everything offline and under your control.

---

## âœ¨ Features

- Simple desktop chat interface
- Connects to a local LLM backend over HTTP
- Conversation history handling
- Clean, minimal UI
- No cloud dependencies
- Fast startup and low overhead

---

## ğŸ›  Requirements

- Java 17 or newer (recommended)
- Maven
- A locally running LLM server (for example, a LLaMA server)

---

## ğŸš€ How to Run

1. Clone the repository:

2. Open the project in IntelliJ IDEA (or your preferred IDE)

3. Make sure Maven dependencies are downloaded

4. Run `Main.java`

Make sure your local LLM server is running before starting the application.

---

## ğŸ“ Project Structure

src/
â””â”€â”€ main/  
â””â”€â”€ java/  
â””â”€â”€ org/example/  
â”œâ”€â”€ Main.java  
â”œâ”€â”€ ChatWindow.java  
â”œâ”€â”€ Conversation.java  
â””â”€â”€ LlamaClient.java  
pom.xml  
.gitignore


---

## ğŸ”’ Privacy

This application is designed to run completely locally. No data is sent to external servers unless you configure it to do so. All conversations stay on your machine.

---

## ğŸ“Œ Notes

- This project is still under development and meant for personal use and learning.
- The code is intentionally kept easy to read and modify.
- Feel free to fork it, experiment with it, or build your own features on top.

---

## ğŸ“„ License

This project is open-source and available under the MIT License.

---

Developed by **Owen**
